# Pensieve Local LLM Server

**A modular local LLM server with Anthropic API compatibility and authentication**

## ğŸ¯ Current Status: **Foundation Complete, Ready for Real Model Integration**

Pensieve provides a **working HTTP API server** with Anthropic API compatibility, proper authentication, and streaming support. The foundation is solid and ready for integrating real LLM models.

### âœ… **What Actually Works (Verified)**

- âœ… **HTTP API Server**: Starts reliably with proper error handling
- âœ… **Authentication**: Bearer token validation (supports test tokens and Anthropic-style keys)
- âœ… **SSE Streaming**: Real Server-Sent Events with proper headers
- âœ… **Mock Responses**: Realistic responses for testing and development
- âœ… **Health Endpoint**: Basic health monitoring
- âœ… **CLI Interface**: Server start/stop with configuration management
- âœ… **Modular Architecture**: 8-crates with clean separation of concerns

### âš ï¸ **What's Next (Not Yet Implemented)**

- âš ï¸ **Real LLM Inference**: Currently uses mock responses only
- âš ï¸ **Model Loading**: Framework ready, but no real model integration yet
- âš ï¸ **GPU Acceleration**: Metal framework integrated but not used for inference

## ğŸš€ Quick Start (Verified Steps)

### Prerequisites

- **Rust 1.75+** (tested with stable)
- **macOS or Linux** (Apple Silicon recommended)
- **4GB+ RAM** (for mock server operation)

### Installation & Testing

**Step 1: Clone and Build**
```bash
# Clone the repository
git clone https://github.com/that-in-rust/pensieve-local-llm-server
cd pensieve-local-llm-server

# Build the project (should compile with only warnings)
cargo build --workspace
```

**Step 2: Create Dummy Model File**
```bash
# The CLI requires a model file for validation
touch model.gguf
```

**Step 3: Start the Server**
```bash
# Start the server on port 8080
cargo run -p pensieve-01 -- start --model ./model.gguf --host 127.0.0.1 --port 8080
```

Expected output:
```
Starting Pensieve server...
Starting server on 127.0.0.1:8080
Server started successfully on 127.0.0.1:8080
Press Ctrl+C to stop the server
```

**Step 4: Test Health Endpoint**
```bash
curl http://127.0.0.1:8080/health
```

Expected response:
```json
{"status":"healthy","timestamp":"2024-01-01T00:00:00Z"}
```

**Step 5: Test Authentication**
```bash
# This should fail with 401 (no authentication)
curl -X POST http://127.0.0.1:8080/v1/messages \
  -H "Content-Type: application/json" \
  -d '{"model": "claude-3-sonnet-20240229", "max_tokens": 10, "messages": [{"role":"user","content":[{"type":"text","text":"Hello"}]}]}'
```

Expected error:
```
Missing request header "authorization"
```

**Step 6: Test API with Authentication**
```bash
# Use valid test token
curl -X POST http://127.0.0.1:8080/v1/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-api-key-12345" \
  -d '{"model": "claude-3-sonnet-20240229", "max_tokens": 50, "messages": [{"role":"user","content":[{"type":"text","text":"Hello Pensieve"}]}]}'
```

Expected response:
```json
{
  "id": "...",
  "type": "message",
  "role": "assistant",
  "content": [{"type": "text", "text": "Mock response to: Hello Pensieve"}],
  "model": "claude-3-sonnet-20240229",
  "stop_reason": "end_turn",
  "stop_sequence": null,
  "usage": {"input_tokens": 10, "output_tokens": 5}
}
```

**Step 7: Test Streaming**
```bash
# Test streaming with authentication
curl -N -X POST http://127.0.0.1:8080/v1/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-ant-api-test123" \
  -H "x-stream: true" \
  -d '{"model": "claude-3-sonnet-20240229", "max_tokens": 10, "messages": [{"role":"user","content":[{"type":"text","text":"Stream test"}]}]}'
```

Expected streaming output:
```
data: {"type": "message_start"}
data: {"type": "content_block_delta", "delta": {"text": "Mock"}}
data: {"type": "message_stop"}
```

## ğŸ“‹ API Reference

### Authentication

The server requires Bearer token authentication via the `Authorization` header:

**Supported Tokens:**
- `test-api-key-12345` (for development/testing)
- `sk-ant-api-*` (Anthropic-style API keys)

### Endpoints

#### `POST /v1/messages` (Non-Streaming)

**Request:**
```bash
curl -X POST http://127.0.0.1:8080/v1/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-api-key-12345" \
  -d '{
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 100,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Hello, world!"
          }
        ]
      }
    ]
  }'
```

#### `POST /v1/messages` (Streaming)

**Request:**
```bash
curl -N -X POST http://127.0.0.1:8080/v1/messages \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer test-api-key-12345" \
  -H "x-stream: true" \
  -d '{
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 50,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Stream this response"
          }
        ]
      }
    ]
  }'
```

#### `GET /health`

**Request:**
```bash
curl http://127.0.0.1:8080/health
```

## ğŸ—ï¸ Architecture

Pensieve follows a **layered architecture** with 8 independent crates:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   pensieve-01   â”‚    â”‚   pensieve-02   â”‚    â”‚   pensieve-03   â”‚
â”‚     CLI Layer   â”‚â—„â”€â”€â–ºâ”‚  HTTP Server   â”‚â—„â”€â”€â–ºâ”‚  API Models     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Config Mgmt   â”‚    â”‚ â€¢ Auth Headers â”‚    â”‚ â€¢ Anthropic API  â”‚
â”‚ â€¢ Commands      â”‚    â”‚ â€¢ Request Routingâ”‚    â”‚ â€¢ JSON Serde     â”‚
â”‚ â€¢ Lifecycle     â”‚    â”‚ â€¢ Streaming     â”‚    â”‚ â€¢ Validation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   pensieve-04   â”‚
                    â”‚ Inference Engineâ”‚
                    â”‚                 â”‚
                    â”‚ â€¢ Candle ML     â”‚
                    â”‚ â€¢ Mock Handler  â”‚
                    â”‚ â€¢ Streaming     â”‚
                    â”‚ â€¢ Performance   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   pensieve-05   â”‚
                    â”‚  Model Support  â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ GGUF Format   â”‚
                    â”‚ â€¢ Data Models   â”‚
                    â”‚ â€¢ Validation    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   pensieve-06   â”‚
                    â”‚  Metal Support  â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ GPU Framework â”‚
                    â”‚ â€¢ Device Mgmt   â”‚
                    â”‚ â€¢ Acceleration  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   pensieve-07   â”‚
                    â”‚ Core Foundation â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ Traits        â”‚
                    â”‚ â€¢ Error Types   â”‚
                    â”‚ â€¢ Resources     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚pensieve-08_claudeâ”‚
                    â”‚ Claude Core     â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ Claude Types  â”‚
                    â”‚ â€¢ Integration   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependency Layers

- **L1 (Core)**: `pensieve-07` - Foundation traits and error types
- **L2 (Engine)**: `pensieve-04`, `pensieve-05`, `pensieve-06` - Core functionality
- **L3 (Application)**: `pensieve-01`, `pensieve-02`, `pensieve-03` - User-facing features

## ğŸ› ï¸ Development

### Building

```bash
# Development build
cargo build

# Release build (optimized)
cargo build --release

# Run specific crate tests
cargo test -p pensieve-01
cargo test -p pensieve-02
```

### CLI Commands

```bash
# Start server with custom configuration
cargo run -p pensieve-01 -- start --model ./model.gguf --host 127.0.0.1 --port 8080

# Show help
cargo run -p pensieve-01 -- --help

# Validate configuration
cargo run -p pensieve-01 -- validate --config config.json
```

## ğŸ§ª Testing

### Current Test Status

The workspace compiles with warnings but has some test compilation issues. The core functionality is verified through manual testing.

```bash
# Build check (works)
cargo check --workspace

# Manual testing recommended (see Quick Start section)
```

## ğŸš§ Next Steps (Roadmap)

### âœ… Completed

- âœ… **Foundation Architecture**: 8-crates with clean dependency hierarchy
- âœ… **HTTP API Server**: Complete with authentication and streaming
- âœ… **Authentication System**: Bearer token validation
- âœ… **Mock Responses**: Realistic behavior for testing
- âœ… **SSE Streaming**: Proper Server-Sent Events implementation
- âœ… **CLI Interface**: Server management and configuration
- âœ… **Error Handling**: Comprehensive error responses

### ğŸ”„ In Progress

- ğŸ”„ **Test Suite**: Fixing compilation errors in test modules
- ğŸ”„ **Documentation**: Improving accuracy and completeness

### ğŸ¯ Future Development

- **Real Model Integration**: Connect to actual GGUF models using Candle
- **Performance Optimization**: GPU acceleration and memory management
- **Advanced Features**: Model switching, configuration management
- **Production Tools**: Monitoring, metrics, deployment guides

## ğŸ¤ Contributing

This project follows **TDD-first principles**:

1. **Write tests first** (when adding new features)
2. **Implement minimal working solution**
3. **Refactor and optimize**
4. **Test manually** (until test suite is fixed)
5. **Update documentation**

### Development Workflow

```bash
# 1. Create feature branch
git checkout -b feature-name

# 2. Write failing test (if applicable)
# 3. Implement minimal solution
# 4. Test manually using curl examples
# 5. Refactor and optimize
# 6. Update README with verified functionality
# 7. Submit pull request
```

## ğŸ“„ License

MIT OR Apache-2.0

## ğŸ™ Acknowledgments

- **Candle ML Framework**: For excellent Rust ML infrastructure
- **Anthropic**: For API compatibility standards
- **Apple Silicon Community**: For Metal optimization guidance

---

## ğŸ¯ Summary

Pensieve Local LLM Server provides a **solid foundation** for local LLM development with:

- âœ… **Working HTTP API** with full Anthropic compatibility
- âœ… **Proper authentication** for secure access
- âœ… **Streaming support** for real-time responses
- âœ… **Modular architecture** for maintainability
- âœ… **Mock responses** for development and testing

The server is **ready for real model integration** and can serve as a foundation for building complete local AI development tools.

---

**Current Status: Foundation Complete, Ready for Model Integration**
**Last Updated: October 28, 2025**
**Version: 0.1.0**