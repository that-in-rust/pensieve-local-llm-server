//! Standalone MLX Performance Tests
//!
//! These tests validate the MLX pipeline without dependencies on problematic crates

use std::process::Command;
use std::path::Path;
use serde_json::Value;

/// Performance contract for MLX inference
pub struct MlxPerformanceContract {
    pub min_tps: f64,
    pub max_memory_mb: f64,
    pub max_latency_ms: u64,
}

impl Default for MlxPerformanceContract {
    fn default() -> Self {
        Self {
            min_tps: 25.0,        // Target: 25+ TPS
            max_memory_mb: 3000.0, // Max 3GB memory
            max_latency_ms: 5000,  // Max 5s for 100 tokens
        }
    }
}

/// Test MLX basic functionality
#[test]
fn test_mlx_basic_functionality() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing MLX Basic Functionality");

    let output = Command::new("python3")
        .arg("python_bridge/mlx_inference.py")
        .arg("--model-path")
        .arg("models/Phi-3-mini-128k-instruct-4bit")
        .arg("--prompt")
        .arg("Hello, world!")
        .arg("--max-tokens")
        .arg("50")
        .arg("--temperature")
        .arg("0.7")
        .arg("--metrics")
        .output()?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("MLX inference failed: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let json_str = stdout.lines().last().unwrap_or("{}");
    let json: Value = serde_json::from_str(json_str)?;

    // Verify basic functionality
    let text = json["text"].as_str().unwrap_or("");
    let tps = json["tokens_per_second"].as_f64().unwrap_or(0.0);
    let memory_mb = json["peak_memory_mb"].as_f64().unwrap_or(0.0);
    let completion_tokens = json["completion_tokens"].as_u64().unwrap_or(0);

    assert!(!text.is_empty(), "Should generate non-empty text");
    assert!(completion_tokens > 0, "Should generate tokens");
    assert!(tps > 0.0, "Should measure TPS");

    println!("‚úÖ Basic functionality test passed:");
    println!("   - Generated {} tokens", completion_tokens);
    println!("   - TPS: {:.1}", tps);
    println!("   - Memory: {:.1} MB", memory_mb);

    Ok(())
}

/// Test MLX performance target
#[test]
fn test_mlx_performance_target() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing MLX Performance Target (25+ TPS)");

    let contract = MlxPerformanceContract::default();

    let output = Command::new("python3")
        .arg("python_bridge/mlx_inference.py")
        .arg("--model-path")
        .arg("models/Phi-3-mini-128k-instruct-4bit")
        .arg("--prompt")
        .arg("Performance test prompt with sufficient length to generate meaningful output.")
        .arg("--max-tokens")
        .arg("100")
        .arg("--temperature")
        .arg("0.7")
        .arg("--metrics")
        .output()?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("MLX inference failed: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let json_str = stdout.lines().last().unwrap_or("{}");
    let json: Value = serde_json::from_str(json_str)?;

    let tps = json["tokens_per_second"].as_f64().unwrap_or(0.0);
    let memory_mb = json["peak_memory_mb"].as_f64().unwrap_or(0.0);
    let latency_ms = json["elapsed_ms"].as_u64().unwrap_or(0);

    println!("üìä Performance Results:");
    println!("   - TPS: {:.1} (target: {})", tps, contract.min_tps);
    println!("   - Memory: {:.1} MB (max: {})", memory_mb, contract.max_memory_mb);
    println!("   - Latency: {} ms (max: {})", latency_ms, contract.max_latency_ms);

    if tps >= contract.min_tps {
        println!("üéâ TPS Target Achieved! ({:.1} >= {})", tps, contract.min_tps);
    } else {
        println!("‚ö†Ô∏è  TPS Below Target ({:.1} < {})", tps, contract.min_tps);
    }

    // Performance assertions
    assert!(tps > 10.0, "Should achieve at least 10 TPS");
    assert!(memory_mb > 0.0, "Should measure memory usage");
    assert!(latency_ms > 0, "Should measure latency");

    Ok(())
}

/// Test MLX model availability
#[test]
fn test_mlx_model_availability() {
    println!("üß™ Testing MLX Model Availability");

    let model_path = Path::new("models/Phi-3-mini-128k-instruct-4bit");
    assert!(model_path.exists(), "Model directory should exist");

    let required_files = vec![
        "config.json",
        "model.safetensors",
        "tokenizer.json"
    ];

    for file in required_files {
        let file_path = model_path.join(file);
        assert!(file_path.exists(), "Required file should exist: {}", file);
    }

    println!("‚úÖ Model files available in: {}", model_path.display());
}

/// Test MLX Python bridge availability
#[test]
fn test_mlx_python_bridge_availability() {
    println!("üß™ Testing MLX Python Bridge Availability");

    let output = Command::new("python3")
        .arg("--version")
        .output();

    assert!(output.is_ok(), "Python should be available");

    let version_output = output.unwrap();
    assert!(version_output.status.success(), "Python version should be retrievable");

    let version_str = String::from_utf8_lossy(&version_output.stdout);
    assert!(version_str.contains("Python 3"), "Should be Python 3");

    println!("‚úÖ Python bridge available: {}", version_str.trim());
}

/// Test MLX dependencies
#[test]
fn test_mlx_dependencies() {
    println!("üß™ Testing MLX Dependencies");

    let test_script = r#"
import sys
try:
    import mlx.core as mx
    import mlx_lm
    print("MLX dependencies available")
    sys.exit(0)
except ImportError as e:
    print(f"MLX dependency missing: {e}")
    sys.exit(1)
"#;

    let output = Command::new("python3")
        .arg("-c")
        .arg(test_script)
        .output();

    assert!(output.is_ok(), "Should be able to run Python script");

    let result = output.unwrap();
    if !result.status.success() {
        println!("‚ö†Ô∏è  MLX dependencies check failed: {}", String::from_utf8_lossy(&result.stderr));
        // Don't fail the test if dependencies are missing in CI
        return;
    }

    let stdout = String::from_utf8_lossy(&result.stdout);
    assert!(stdout.contains("MLX dependencies available"));

    println!("‚úÖ MLX dependencies available");
}

/// Test MLX cache performance
#[test]
fn test_mlx_cache_performance() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing MLX Cache Performance");

    // First request (cache miss)
    let output1 = Command::new("python3")
        .arg("python_bridge/mlx_inference.py")
        .arg("--model-path")
        .arg("models/Phi-3-mini-128k-instruct-4bit")
        .arg("--prompt")
        .arg("Cache test prompt")
        .arg("--max-tokens")
        .arg("50")
        .arg("--temperature")
        .arg("0.7")
        .arg("--metrics")
        .output()?;

    // Second request (potential cache hit)
    let output2 = Command::new("python3")
        .arg("python_bridge/mlx_inference.py")
        .arg("--model-path")
        .arg("models/Phi-3-mini-128k-instruct-4bit")
        .arg("--prompt")
        .arg("Different cache test prompt")
        .arg("--max-tokens")
        .arg("50")
        .arg("--temperature")
        .arg("0.7")
        .arg("--metrics")
        .output()?;

    assert!(output1.status.success(), "First request should succeed");
    assert!(output2.status.success(), "Second request should succeed");

    let stdout1 = String::from_utf8_lossy(&output1.stdout);
    let json_str1 = stdout1.lines().last().unwrap_or("{}");
    let json1: Value = serde_json::from_str(json_str1)?;

    let stdout2 = String::from_utf8_lossy(&output2.stdout);
    let json_str2 = stdout2.lines().last().unwrap_or("{}");
    let json2: Value = serde_json::from_str(json_str2)?;

    let tps1 = json1["tokens_per_second"].as_f64().unwrap_or(0.0);
    let tps2 = json2["tokens_per_second"].as_f64().unwrap_or(0.0);

    let cache_hit_rate1 = json1["performance_metrics"]["cache_hit_rate"].as_f64().unwrap_or(0.0);
    let cache_hit_rate2 = json2["performance_metrics"]["cache_hit_rate"].as_f64().unwrap_or(0.0);

    println!("üìä Cache Performance:");
    println!("   - Request 1 TPS: {:.1}", tps1);
    println!("   - Request 2 TPS: {:.1}", tps2);
    println!("   - Request 1 Cache Hit: {:.1%}", cache_hit_rate1);
    println!("   - Request 2 Cache Hit: {:.1%}", cache_hit_rate2);

    // Both requests should work regardless of cache performance
    assert!(tps1 > 0.0, "First request should work");
    assert!(tps2 > 0.0, "Second request should work");

    // Cache hit rate should be reasonable (0.0 to 1.0)
    assert!(cache_hit_rate1 >= 0.0 && cache_hit_rate1 <= 1.0);
    assert!(cache_hit_rate2 >= 0.0 && cache_hit_rate2 <= 1.0);

    Ok(())
}

/// Test MLX streaming functionality
#[test]
fn test_mlx_streaming_functionality() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing MLX Streaming Functionality");

    let output = Command::new("python3")
        .arg("python_bridge/mlx_inference.py")
        .arg("--model-path")
        .arg("models/Phi-3-mini-128k-instruct-4bit")
        .arg("--prompt")
        .arg("Streaming test prompt")
        .arg("--max-tokens")
        .arg("50")
        .arg("--temperature")
        .arg("0.7")
        .arg("--stream")
        .output()?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("MLX streaming failed: {}", stderr).into());
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let lines: Vec<&str> = stdout.lines().collect();

    println!("üìä Streaming Results:");
    println!("   - Lines received: {}", lines.len());

    // Should receive multiple lines for streaming
    assert!(lines.len() > 1, "Should receive multiple lines for streaming");

    // Check that at least some lines contain JSON
    let json_lines = lines.iter().filter(|line| line.starts_with('{')).count();
    assert!(json_lines > 0, "Should receive JSON responses");

    println!("   - JSON lines: {}", json_lines);
    println!("‚úÖ Streaming functionality test passed");

    Ok(())
}

/// Test MLX performance regression
#[test]
fn test_mlx_performance_regression() -> Result<(), Box<dyn std::error::Error>> {
    println!("üß™ Testing MLX Performance Regression");

    let contract = MlxPerformanceContract::default();

    // Run multiple requests to check for consistent performance
    let mut tps_values = Vec::new();
    let mut memory_values = Vec::new();

    for i in 0..3 {
        let output = Command::new("python3")
            .arg("python_bridge/mlx_inference.py")
            .arg("--model-path")
            .arg("models/Phi-3-mini-128k-instruct-4bit")
            .arg("--prompt")
            .arg(&format!("Regression test prompt {}", i))
            .arg("--max-tokens")
            .arg("50")
            .arg("--temperature")
            .arg("0.7")
            .arg("--metrics")
            .output()?;

        assert!(output.status.success(), "Request {} should succeed", i);

        let stdout = String::from_utf8_lossy(&output.stdout);
        let json_str = stdout.lines().last().unwrap_or("{}");
        let json: Value = serde_json::from_str(json_str)?;

        let tps = json["tokens_per_second"].as_f64().unwrap_or(0.0);
        let memory_mb = json["peak_memory_mb"].as_f64().unwrap_or(0.0);

        tps_values.push(tps);
        memory_values.push(memory_mb);
    }

    // Calculate average TPS
    let avg_tps = tps_values.iter().sum::<f64>() / tps_values.len() as f64;
    let avg_memory = memory_values.iter().sum::<f64>() / memory_values.len() as f64;

    println!("üìä Regression Test Results:");
    println!("   - TPS values: {:?}", tps_values);
    println!("   - Average TPS: {:.1}", avg_tps);
    println!("   - Memory values: {:?}", memory_values.iter().map(|m| format!("{:.1}", m)).collect::<Vec<_>>());
    println!("   - Average Memory: {:.1} MB", avg_memory);

    // Performance regression checks
    assert!(avg_tps > 10.0, "Average TPS should be reasonable");
    assert!(avg_memory > 1000.0, "Should use reasonable memory for model");
    assert!(avg_memory < 5000.0, "Should not exceed memory limits");

    // Check performance consistency (TPS shouldn't vary too much)
    let tps_variance = tps_values.iter().map(|tps| (tps - avg_tps).powi(2)).sum::<f64>() / tps_values.len() as f64;
    let tps_std_dev = tps_variance.sqrt();

    println!("   - TPS Standard Deviation: {:.1}", tps_std_dev);
    assert!(tps_std_dev < avg_tps, "TPS should be reasonably consistent");

    Ok(())
}