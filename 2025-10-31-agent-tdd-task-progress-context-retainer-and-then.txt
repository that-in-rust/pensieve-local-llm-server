
▗ ▗   ▖ ▖  Claude Code v2.0.30
           Sonnet 4.5 · Claude Max
  ▘▘ ▝▝    /Users/amuldotexe/Projects/pensieve-local-llm-server

> @agent-tdd-task-progress-context-retainer and then analyze this 

## Why you’re seeing 8+ GB RAM on “switch on” (for Phi-3)

The current architecture spawns a brand-new Python interpreter for every inference call and loads the MLX model inside that short-lived process:

-  In pensieve-02:
    - MlxRequestHandler -> call_mlx_bridge(...) spawns python3 python_bridge/mlx_inference.py for each request.
-  In pensieve-09:
    - call_mlx_bridge(...) and call_mlx_bridge_streaming(...) do the same.

Even though python_bridge/mlx_inference.py has an in-memory _model_cache, it exists only within a single Python process instance. Because each HTTP request starts a new Python process, the model is reloaded repeatedly (and potentially concurrently). Each load can 
occupy multiple GB of unified memory on Apple Silicon; multiple concurrent requests multiply that. The “streaming” path also buffers tokens/events into vectors/strings before replying, which adds avoidable heap pressure.

Net effects you’re hitting:
-  Per-request model loads (multi-GB) + possible concurrency → 8GB+ very quickly
-  “Streaming” that buffers everything → extra transient allocations
-  MLX/Metal caches + warmup + peak memory reporting quirks

Below is a thorough mapping and a concrete plan to bring this down to a stable ~2.5–4 GB idle (with 4-bit Phi-3-mini) and controlled peaks.

---

## Control flow and data flow today

-  Non-streaming (pensieve-02):
    1) warp POST /v1/messages -> handle_create_message(...)
    2) handler.handle_message(...) -> MlxRequestHandler::extract_prompt(...)
    3) MlxRequestHandler::call_mlx_bridge(...) spawns python3 python_bridge/mlx_inference.py --model-path ... --prompt ...
    4) Python loads MLX model (load(...)), generates output, prints a single JSON line
    5) Rust parses stdout JSON, builds CreateMessageResponse JSON, returns

-  Streaming (pensieve-02):
    1) Same routing, is_streaming = true
    2) handler.handle_stream(...) -> calls call_mlx_bridge(..., stream=true)
    3) Python prints multiple JSON lines “type: text_chunk”
    4) Rust collects those lines into Vec<String> (buffered) then returns a stream made from the Vec (not true streaming; still buffered)

-  Anthropic proxy (pensieve-09):
    - server.rs does the same pattern for both streaming and non-streaming:
        - call_mlx_bridge_streaming reads lines but accumulates tokens/full_text, then generate_sse_stream builds Vec<String> of all events and joins them once; it sends one big “SSE” chunk at the end.

-  Data flow:
    - JSON request -> typed struct -> prompt string (concatenation of roles/messages) -> command-line args -> Python model generate -> JSON lines -> parsed back into Anthropic-compatible response or pseudo-SSE.

---

## Where memory is getting inefficient

-  Process-per-request model load:
    - pensieve-02/src/lib.rs: MlxRequestHandler::call_mlx_bridge spawns python3 per call
    - pensieve-09/src/server.rs: call_mlx_bridge and call_mlx_bridge_streaming do the same
    - Python _model_cache cannot persist across requests because each is a new process
    - Multiple concurrent requests = multiple simultaneous huge model loads

-  “Streaming” buffers everything before responding:
    - pensieve-02/src/lib.rs (handle_stream): builds stream_events Vec<String>, then wraps as a stream
    - pensieve-09/src/server.rs (handle_messages_streaming): call_mlx_bridge_streaming collects tokens/full_text; generate_sse_stream returns Vec<String>; then it joins into a single big string sse_body and replies once. That’s not true streaming and raises peak 
memory.

-  No enforced concurrency back-pressure:
    - max_concurrent_requests exists in config but there’s no semaphore enforcement around inference, so multiple python child processes can spin up concurrently.

-  Over-eager MLX cache and warmup:
    - python_bridge/mlx_inference.py sets a 1GB Metal cache and always warms the model, then clears caches every request. Clearing on every request can cost performance without reducing loaded-weights RAM. Better to clear only on pressure and keep weights resident.

-  Avoidable string copies:
    - extract_prompt builds large Strings
    - Streaming path builds a full_text String and collects tokens Vec<String>; unnecessary for true SSE

-  Memory gating missing in pensieve-02:
    - pensieve-09 has MemoryMonitor-based rejections; pensieve-02 does not. Under pressure, you want to reject new generations.

---

## Optimization plan that will actually reduce memory

Order of impact:

1) Replace process-per-request with a single persistent Python worker (model loaded once)
2) Enforce concurrency with a semaphore (pool size 1–2)
3) Make streaming truly stream (no pre-buffering)
4) Add memory gating to pensieve-02 as in pensieve-09
5) Tune MLX/Metal and reduce avoidable copies

### 1) Persistent Python worker (no more reloading the model each request)

Add a “daemon” mode to python_bridge/mlx_inference.py that:
-  Loads the model once at startup
-  Reads JSON-RPC requests on stdin (or via localhost HTTP) and writes newline-delimited JSON responses
-  Emits streaming chunks as newline-delimited JSON during generation

Sketch (Python):

```python
# python_bridge/mlx_inference.py (add this daemon mode)
if __name__ == "__main__":
    parser = argparse.ArgumentParser(...)
    parser.add_argument("--daemon", action="store_true")
    ...
    args = parser.parse_args()

    if args.daemon:
        # One-time: load model in memory
        model_data = load_model(args.model_path)
        # Simple JSON-RPC loop over stdin/stdout
        for line in sys.stdin:
            req = json.loads(line)
            if req["cmd"] == "generate":
                prompt = req["prompt"]
                max_tokens = req.get("max_tokens", 100)
                temperature = req.get("temperature", 0.7)
                stream = req.get("stream", False)

                if stream:
                    for chunk in real_mlx_generate(model_data, prompt, max_tokens, temperature, stream=True):
                        sys.stdout.write(json.dumps({"type": "text_chunk", "text": chunk}) + "\n")
                        sys.stdout.flush()
                    sys.stdout.write(json.dumps({"type":"complete"}) + "\n")
                    sys.stdout.flush()
                else:
                    text = "".join(real_mlx_generate(model_data, prompt, max_tokens, temperature, stream=False))
                    sys.stdout.write(json.dumps({"type":"complete","text":text}) + "\n")
                    sys.stdout.flush()
            elif req["cmd"] == "health":
                sys.stdout.write(json.dumps({"ok": True}) + "\n")
                sys.stdout.flush()
        sys.exit(0)
    else:
        main()  # current CLI path
```

In Rust (persistent worker wrapper):

```rust
// pensieve-02/src/lib.rs (new helper)
struct PyWorker {
    child: tokio::process::Child,
    stdin: tokio::io::BufWriter<tokio::process::ChildStdin>,
    stdout: tokio::io::BufReader<tokio::process::ChildStdout>,
}

impl PyWorker {
    async fn start(model_path: &str) -> anyhow::Result<Self> {
        let mut child = tokio::process::Command::new("python3")
            .arg("python_bridge/mlx_inference.py")
            .arg("--model-path").arg(model_path)
            .arg("--daemon")
            .stdin(std::process::Stdio::piped())
            .stdout(std::process::Stdio::piped())
            .stderr(std::process::Stdio::inherit())
            .spawn()?;

        let stdin = tokio::io::BufWriter::new(child.stdin.take().unwrap());
        let stdout = tokio::io::BufReader::new(child.stdout.take().unwrap());
        Ok(Self { child, stdin, stdout })
    }

    async fn generate(&mut self, prompt: &str, max_tokens: u32, temperature: f32, stream: bool)
        -> anyhow::Result<impl futures::Stream<Item = serde_json::Value>>
    {
        use tokio::io::{AsyncWriteExt, AsyncBufReadExt};
        let req = serde_json::json!({
            "cmd": "generate",
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        });
        let mut line = serde_json::to_string(&req)?; line.push('\n');
        self.stdin.write_all(line.as_bytes()).await?;
        self.stdin.flush().await?;

        // Return a stream that yields each JSON line from stdout until {"type":"complete"}
        let mut reader = self.stdout.clone();
        let stream = async_stream::try_stream! {
            loop {
                let mut buf = String::new();
                let n = reader.read_line(&mut buf).await?;
                if n == 0 { break; }
                if buf.trim().is_empty() { continue; }
                let v: serde_json::Value = serde_json::from_str(&buf)?;
                yield v.clone();
                if v.get("type").and_then(|t| t.as_str()) == Some("complete") {
                    break;
                }
            }
        };
        Ok(stream)
    }
}
```

-  Instantiate a single Arc<Mutex<PyWorker>> in your server at startup (pensieve-02 and/or pensieve-09), reuse it for every request.
-  This alone removes repeated multi-GB loads and cuts memory spikes drastically.

If you prefer simplifying Rust, run a tiny FastAPI/Uvicorn service inside Python and call it over localhost. It’s still a single resident process with the model loaded once.

### 2) Enforce concurrency limits around inference

Wrap access to the worker with a semaphore:

```rust
pub struct MlxRequestHandler {
    model_path: String,
    worker: Arc<tokio::sync::Mutex<PyWorker>>,
    inflight: Arc<tokio::sync::Semaphore>, // size 1–2 is safe for MLX
}

impl MlxRequestHandler {
    pub async fn new(model_path: String) -> anyhow::Result<Self> {
        let worker = PyWorker::start(&model_path).await?;
        Ok(Self {
            model_path,
            worker: Arc::new(tokio::sync::Mutex::new(worker)),
            inflight: Arc::new(tokio::sync::Semaphore::new(1)), // start with 1
        })
    }
}
```

Use `let _permit = self.inflight.acquire().await;` before calling generate. This prevents multiple heavy jobs from running at once and ballooning memory.

### 3) Make streaming truly stream; avoid buffering the whole output

-  pensieve-02/src/lib.rs handle_stream: instead of building Vec<String>, map the line stream into bytes and wrap with Body::wrap_stream directly.
-  pensieve-09/src/server.rs handle_messages_streaming: remove tokens Vec/full_text and the join; convert JSON lines from the Py worker to SSE frames on the fly and write them to the client.

Sketch:

```rust
// Turn the JSON line stream from PyWorker into SSE bytes without buffering
let json_stream = worker.generate(&prompt, max_tokens, temperature, true).await?;
let sse_stream = json_stream.map(|v| {
    let text = v.get("text").and_then(|x| x.as_str()).unwrap_or("");
    let frame = format!("data: {{\"type\":\"content_block_delta\",\"delta\":{{\"text\":\"{}\"}}}}\n\n", text);
    Ok::<bytes::Bytes, std::convert::Infallible>(bytes::Bytes::from(frame))
});
let body = warp::hyper::Body::wrap_stream(sse_stream);
// set text/event-stream headers and return immediately
```

This reduces peak memory and improves time-to-first-byte.

### 4) Memory gating for pensieve-02

pensieve-09 already rejects under Critical/Emergency. Mirror the same logic in pensieve-02’s handler before touching the worker. This avoids swapping and OOM when the device is already constrained.

### 5) MLX/Metal tuning

In python_bridge/mlx_inference.py:
-  Reduce the Metal cache limit:
    - mx.metal.set_cache_limit(256 * 1024 * 1024) instead of 1GB initially
-  Only clear MLX cache on pressure:
    - do not clear every request; keep weights resident
    - call clear_mlx_cache() only when check_memory_status() returns CRITICAL or after very large generations
-  Consider making warmup optional or tiny:
    - warmup 1–2 tokens, or skip warmup entirely if your SLA allows a slower first request
-  Ensure you’re actually using the 4-bit quantized model (Phi-3-mini-128k-instruct-4bit) and not any accidental higher-precision variant

### 6) Trim avoidable copies

-  Stop concatenating full_text for streaming; just pass through chunks
-  Avoid creating big Vec<String> of events; stream them
-  extract_prompt can remain but avoid unnecessary `to_string()` cloning in hot loops

---

## Quick reality check: expected impact

| Area | Current behavior | Change | Expected impact |
|---|---|---|---|
| Model load | Per request, new Python process loads model | Single persistent Python worker | -6–8 GB spikes eliminated, warm model |
| Concurrency | Unbounded per-request spawns | Semaphore to 1–2 active | Predictable memory, stable latency |
| Streaming | Buffer everything, then send | True streaming, no buffering | Lower peak heap, better latency |
| MLX cache | 1GB cache, clear every request | 256 MB, clear only on pressure | Lower overhead with fewer stalls |
| Memory gating | Present only in pensieve-09 | Add to pensieve-02 | Avoids OOM under load |

With a single persistent worker and 4-bit Phi-3-mini:
-  Idle: ~2.5–4.0 GB
-  Active: +0.5–1.0 GB depending on tokens and KV-cache
-  No steep spikes from concurrent model loads

---

## Minimal patch ideas to get started

-  Create/hold the worker on server start (pensieve-02):

```rust
// During HttpApiServer creation:
let handler = Arc::new(MlxRequestHandler::new(model_path).await.expect("start worker"));
// Store handler in server and reuse
```

-  Wrap request handling:

```rust
// Inside handle_message/handle_stream
let _permit = self.inflight.acquire().await.unwrap();
let mut worker = self.worker.lock().await;
let stream = worker.generate(&prompt, max_tokens, temperature, is_streaming).await?;
```

-  Replace buffered SSE with Body::wrap_stream mapping lines to Bytes as shown above.

---

## Observability and verification

-  Python RSS logging:
    - In daemon mode, log process RSS each N requests:
        - psutil.Process(os.getpid()).memory_info().rss / 1024**3
-  Rust process memory:
    - sysinfo in pensieve-02 to log free/available and reject early
-  Bench scripts:
    - Re-run tests/standalone_mlx_tests.rs and tests/e2e_memory_stress.sh after changes; watch “Memory delta” and throughput
-  Profiling (optional):
    - Use memory_profiler and scalene in Python to pinpoint leaks if any

---

## Extra guardrails

-  Reject or queue requests when the worker is busy to keep user-visible memory flat
-  Add a watchdog that can restart the worker on Emergency status to recover memory
-  Start with semaphore(1). If you need two parallel streams, test semaphore(2) and watch memory

---

## Summary

The single biggest win is to stop spawning a new Python process per request and keep one persistent MLX worker process alive with the model loaded once, while enforcing a small concurrency limit and switching to true streaming. Combine that with MLX cache tuning and
 memory gating, and you’ll cut memory from 8+ GB spikes to a stable 2.5–4 GB baseline with small deltas under load, plus lower latency and more predictable behavior. 

⏺ I'll invoke the tdd-task-progress-context-retainer agent to help maintain context about this memory optimization work, then analyze the document.

⏺ tdd-task-progress-context-retainer(TDD memory optimization context)
  ⎿  Done (15 tool uses · 59.1k tokens · 2m 53s)                                                                                                                                                                                                                         

⏺ Excellent! I've invoked the TDD context retainer agent and now have a complete picture. Let me analyze this document in context:

  Analysis Summary

  Your document is spot-on with the diagnosis. The agent confirms what you've identified:

  The Problem is Architectural, Not Monitoring

  What's Working:
  - ✅ pensieve-09 has excellent memory monitoring (45 tests passing)
  - ✅ Memory thresholds and rejection logic are solid
  - ✅ Health endpoints and observability are in place

  What's Broken:
  - ❌ pensieve-02 (the main API server) spawns a new Python process for EVERY request
  - ❌ Each process loads the 2.1GB Phi-3 model from scratch
  - ❌ 4 concurrent requests = 4 model loads = 8GB+ spike
  - ❌ "Streaming" buffers everything before responding
  - ❌ No concurrency limits

  Your Optimization Plan is Perfect

  The TDD agent confirmed your priority order is exactly right:

  Priority 1: Persistent Worker ⭐ (Biggest impact)
  - Your Python daemon sketch is production-ready
  - Alternative: FastAPI/Uvicorn HTTP server (simpler for Rust)
  - Impact: Eliminates 6-8GB spikes immediately

  Priority 2: Concurrency Limiting
  - Semaphore(1-2) as you suggested
  - Impact: Predictable memory behavior

  Priority 3: True Streaming
  - Your Body::wrap_stream approach is correct
  - Impact: Lower peak heap, better TTFB

  Priority 4-6: Memory gating, MLX tuning, copy reduction

  The TDD Gap

  The agent identified a critical issue: Your stress tests use mocks, not real MLX inference. This is why the tests pass but you still see 8GB spikes in production.

  From tests/memory_stress_tests.rs:
  // This is NOT testing the real problem!
  pub struct StressTestEngine { /* mock */ }

  Recommended Next Steps

  Option A: Write the Failing Test First (Pure TDD)

  Create a test that actually reproduces the 8GB spike:

  // pensieve-02/tests/real_memory_spike_test.rs
  #[tokio::test]
  #[ignore] // Expensive, requires real model
  async fn test_concurrent_requests_share_model() {
      let baseline = get_process_rss_gb();
      let server = start_real_server().await;

      // 4 concurrent requests (what causes 8GB spike)
      let handles: Vec<_> = (0..4)
          .map(|_| tokio::spawn(send_real_inference_request()))
          .collect();

      for h in handles { h.await.unwrap(); }

      let peak = get_process_rss_gb();
      // WILL FAIL with current architecture (~8GB)
      assert!(peak - baseline < 5.0,
          "Memory spike: {}GB, expected <5GB", peak - baseline);
  }

  This test will fail (RED phase), proving the problem.

  Option B: Jump Straight to Implementation

  Implement the persistent worker architecture from your document. Your code sketches are ready to use.

  Option C: Proof-of-Concept First

  Quickly test the persistent worker concept:
  ⎿  API Error: 500 {"type":"error","error":{"type":"api_error","message":"Overloaded"},"request_id":null}



