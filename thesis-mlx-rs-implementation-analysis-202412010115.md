# Thesis: MLX-RS Implementation Analysis for PRD01

**Document ID:** thesis-mlx-rs-implementation-analysis-202412010115
**Generated:** 2024-12-01T01:15:00Z
**Objective:** Rigorous analysis of using mlx-rs for pure Rust LLM inference on Apple Silicon
**Methodology:** 4-agent deep analysis (Explore, Plan, General-Purpose)

---

## Executive Summary

### Target Architecture
```
pensieve-local-llm-server (single binary)
    â”‚
    â”œâ”€â”€ HTTP Server (Rust/Warp)
    â”‚       â”‚
    â”‚       â””â”€â”€ SSE Streaming
    â”‚
    â””â”€â”€ mlx-rs (Rust bindings to MLX)
            â”‚
            â””â”€â”€ MLX C API â†’ MLX Core â†’ Metal/Neural Engine
```

### Verdict

| Criterion | Assessment |
|-----------|------------|
| **Feasibility** | YES - with significant development effort |
| **Timeline** | 8-10 weeks for production-ready implementation |
| **Risk Level** | MEDIUM - mlx-rs is v0.25.x, still maturing |
| **Missing Components** | 4 critical, 3 high priority |
| **Recommended Approach** | Hybrid: mlx-rs core + port missing pieces from Candle |

---

## Part 1: What MLX-RS Provides (90% Complete)

### 1.1 Core Tensor API âœ… COMPLETE

**Location:** `refRepo/mlx-rs/mlx-rs/src/`

| Component | Status | Lines of Code |
|-----------|--------|---------------|
| Array (Tensor) type | âœ… Complete | 800+ |
| Arithmetic ops | âœ… Complete | 2,883 |
| Reduction ops | âœ… Complete | 1,135 |
| Shape ops | âœ… Complete | 1,463 |
| Logical ops | âœ… Complete | 1,125 |
| **Total ops** | âœ… Complete | **9,654** |

**Key Operations Available:**
```rust
// All required for LLM inference
matmul()           âœ…
softmax()          âœ…
layer_norm()       âœ… (via nn::LayerNorm)
embedding()        âœ… (via nn::Embedding)
transpose()        âœ…
reshape()          âœ…
concatenate()      âœ…
```

### 1.2 Neural Network Module âœ… COMPLETE

**Location:** `refRepo/mlx-rs/mlx-rs/src/nn/`

| Layer Type | Status | Notes |
|------------|--------|-------|
| Linear | âœ… | With optional bias |
| Embedding | âœ… | Token embeddings |
| LayerNorm | âœ… | Configurable eps |
| RmsNorm | âœ… | For Phi/Llama style |
| MultiHeadAttention | âœ… | With causal mask |
| Rope (RoPE) | âœ… | Rotary embeddings |
| Transformer | âœ… | Full block |
| Dropout | âœ… | Training mode |
| Conv1d/2d/3d | âœ… | All variants |

**Activation Functions (24 total):**
```
sigmoid, relu, leaky_relu, elu, relu6, softplus, softsign, celu,
silu, gelu, gelu_approximate, gelu_fast_approximate, log_softmax,
log_sigmoid, prelu, mish, hard_swish, selu, step, glu
```

### 1.3 Quantization âœ… COMPLETE

**Location:** `refRepo/mlx-rs/mlx-rs/src/nn/quantized.rs` (384 lines)

```rust
// 4-bit quantization with group_size=64 (PRD01 requirement)
pub trait Quantizable {
    const DEFAULT_GROUP_SIZE: i32 = 64;
    const DEFAULT_BITS: i32 = 4;
    fn try_into_quantized(self, group_size, bits) -> Result<Self::Quantized>
}

// Available quantized layers
QuantizedLinear      âœ…
QuantizedEmbedding   âœ…
MaybeQuantized<T>    âœ…  // Transparent wrapper
nn::quantize()       âœ…  // Generic quantization function
```

### 1.4 Model Loading âœ… SAFETENSORS ONLY

**Location:** `refRepo/mlx-rs/mlx-rs/src/array/safetensors.rs`

```rust
// What works
Array::load_safetensors(path) -> HashMap<String, Array>  âœ…
module.load_safetensors(path)                            âœ…
module.save_safetensors(path)                            âœ…

// What's missing
GGUF format loading                                      âŒ
```

### 1.5 Fast Operations âœ… COMPLETE

**Location:** `refRepo/mlx-rs/mlx-rs/src/fast.rs`

```rust
// C++ backend optimized operations
scaled_dot_product_attention()  âœ…  // Optimized attention
rope()                          âœ…  // Fast RoPE
rms_norm()                      âœ…  // Fast RMS normalization
```

### 1.6 Lazy Evaluation & Streams âœ… COMPLETE

**Location:** `refRepo/mlx-rs/mlx-rs/src/stream.rs`

```rust
// GPU/CPU stream management
StreamOrDevice::cpu()           âœ…
StreamOrDevice::gpu()           âœ…
Array::eval()                   âœ…  // Trigger computation
transforms::eval(&[arrays])     âœ…  // Batch evaluation
```

---

## Part 2: What's MISSING for PRD01 (Critical Gaps)

### 2.1 CRITICAL: No Phi-4 Model Implementation

**Severity:** ðŸ”´ CRITICAL
**Impact:** Cannot run Phi-4 without custom implementation
**Effort:** 600-800 lines of Rust

**Current State:**
- mlx-rs has only **Qwen3** model implemented
- No Phi, Llama, Mistral models in mlx-rs
- Each model requires custom implementation

**Reference for Porting:**
```
Candle Phi:     refRepo/candle/candle-transformers/src/models/phi.rs (365 LOC)
Candle Q-Phi:   refRepo/candle/candle-transformers/src/models/quantized_phi.rs (305 LOC)
mlx-rs Qwen3:   refRepo/mlx-rs/mlx-lm/src/models/qwen3.rs (695 LOC)
```

**Phi-4 Architecture Requirements:**
```rust
struct Phi4Config {
    hidden_size: 4096,
    num_hidden_layers: 32,
    num_attention_heads: 32,
    num_key_value_heads: 8,       // GQA
    intermediate_size: 14336,
    vocab_size: 100352,
    rope_theta: 250000.0,
    rms_norm_eps: 1e-5,
}
```

### 2.2 CRITICAL: No GGUF Format Support

**Severity:** ðŸ”´ CRITICAL
**Impact:** Cannot load GGUF quantized models directly
**Effort:** 500-800 lines OR use offline conversion

**Options:**
1. **Port GGUF parser from Candle** (500-800 LOC)
   - Source: `refRepo/candle/candle-core/src/quantized/gguf_file.rs`

2. **Offline GGUF â†’ Safetensors conversion** (Recommended)
   - Use Python mlx-lm to convert once
   - Load safetensors in mlx-rs

3. **Use mlx-community pre-converted models**
   - HuggingFace has safetensors versions

### 2.3 HIGH: Missing Top-K/Top-P Sampling

**Severity:** ðŸŸ  HIGH
**Impact:** Limited sampling quality (only temperature/argmax)
**Effort:** 300-400 lines

**Current mlx-rs sampler:**
```rust
// refRepo/mlx-rs/mlx-lm/src/sampler.rs (ONLY 20 lines!)
pub struct DefaultSampler;

impl Sampling for DefaultSampler {
    fn sample(&self, logits: &Array, temp: f32) -> Result<Array> {
        if temp == 0.0 {
            logits.argmax(-1, None)  // Greedy only
        } else {
            categorical!(logits / temp)  // Temperature only
        }
    }
}
```

**What's Missing:**
- Top-K filtering
- Top-P (nucleus) sampling
- Repeat penalty
- Presence/frequency penalty

**Candle Reference:**
```
refRepo/candle/candle-transformers/src/generation/mod.rs (200+ LOC)
- TopK, TopP, TopKThenTopP, GumbelSoftmax sampling modes
```

### 2.4 HIGH: No HTTP/SSE Integration

**Severity:** ðŸŸ  HIGH
**Impact:** Requires custom async wrapper for Warp
**Effort:** 500-800 lines

**mlx-rs is synchronous:**
```rust
// Current pattern - blocking iterator
impl Iterator for Generate<M> {
    fn next(&mut self) -> Option<Result<Array>>  // Blocks!
}
```

**Required pattern for Warp SSE:**
```rust
// Need async wrapper with channel
async fn generate_stream() -> impl Stream<Item = SSEEvent> {
    let (tx, rx) = tokio::sync::mpsc::unbounded_channel();

    tokio::spawn_blocking(move || {
        for token in mlx_generator {
            tx.send(token).ok();
        }
    });

    rx
}
```

### 2.5 MEDIUM: KV Cache Interface Differences

**Severity:** ðŸŸ¡ MEDIUM
**Impact:** Need adapter for mlx-rs trait-based cache
**Effort:** 100-200 lines

**mlx-rs uses trait-based cache:**
```rust
// refRepo/mlx-rs/mlx-lm/src/cache.rs
pub trait KeyValueCache {
    fn offset(&self) -> i32;
    fn max_size(&self) -> Option<i32>;
    fn update_and_fetch(&mut self, k: Array, v: Array) -> Result<(Array, Array)>;
}
```

**Candle uses simple tuple:**
```rust
// Candle approach
kv_cache: Option<(Tensor, Tensor)>
```

### 2.6 MEDIUM: No Async Runtime Integration

**Severity:** ðŸŸ¡ MEDIUM
**Impact:** Blocking inference in async context
**Effort:** 200-300 lines

**mlx-rs has no tokio/async-std:**
- All operations are synchronous
- Evaluation blocks the thread
- Need spawn_blocking wrapper

---

## Part 3: Gap Analysis Summary

### Component Comparison Matrix

| Component | Candle | mlx-rs | Gap | Port Effort |
|-----------|--------|--------|-----|-------------|
| Tensor Ops | âœ… Complete | âœ… Complete | None | 0 |
| NN Layers | âœ… Complete | âœ… Complete | None | 0 |
| Quantization | âœ… GGUF native | âœ… Generic 4-bit | Format | Medium |
| Phi Model | âœ… Full + Quantized | âŒ Missing | **Critical** | 600-800 LOC |
| GGUF Loading | âœ… Native | âŒ Missing | Critical | 500-800 LOC |
| Sampling | âœ… TopK/TopP | âš ï¸ Temp only | High | 300-400 LOC |
| Attention | âœ… Manual | âœ… Optimized | None | 0 |
| RoPE | âœ… Partial | âœ… Fast | None | 0 |
| KV Cache | âœ… Tuple | âœ… Trait | Adapter | 100-200 LOC |
| HTTP/SSE | âŒ None | âŒ None | Both need | 500-800 LOC |
| Async | âŒ None | âŒ None | Both need | 200-300 LOC |

### Priority Roadmap

```
PHASE 1 (Week 1-2): Foundation
â”œâ”€â”€ Port Phi-4 model architecture from Candle
â”œâ”€â”€ Implement Phi4Config, Phi4Attention, Phi4MLP, Phi4Block
â””â”€â”€ Use Qwen3 as structural reference

PHASE 2 (Week 3): Model Loading
â”œâ”€â”€ Download safetensors Phi-4 from mlx-community
â”œâ”€â”€ Implement weight loading with ModuleParametersExt
â””â”€â”€ OR: Offline GGUF â†’ safetensors conversion

PHASE 3 (Week 4): Sampling
â”œâ”€â”€ Port Top-K from Candle
â”œâ”€â”€ Port Top-P from Candle
â””â”€â”€ Add repeat penalty

PHASE 4 (Week 5): Generation
â”œâ”€â”€ Implement KV cache adapter
â”œâ”€â”€ Create Phi4Generator iterator
â””â”€â”€ Add token streaming

PHASE 5 (Week 6): HTTP Integration
â”œâ”€â”€ Create async wrapper for mlx-rs
â”œâ”€â”€ Integrate with Warp SSE
â””â”€â”€ Wire to p02-http-server-core

PHASE 6 (Week 7-8): Testing & Optimization
â”œâ”€â”€ Unit tests for all layers
â”œâ”€â”€ Integration tests for generation
â”œâ”€â”€ Performance benchmarks (>=20 TPS target)
â””â”€â”€ Memory profiling (<5GB target)
```

---

## Part 4: Implementation Specifications

### 4.1 Phi-4 Model Structure

```rust
// NEW FILE: p04-inference-engine-core/src/models/phi4.rs

use mlx_rs::{
    macros::{ModuleParameters, Quantizable},
    module::Module,
    nn::{self, Linear, RmsNorm, Embedding, Rope},
    quantization::MaybeQuantized,
    Array,
};

#[derive(Debug, Clone, serde::Deserialize)]
pub struct Phi4Config {
    pub hidden_size: i32,           // 4096
    pub num_hidden_layers: i32,     // 32
    pub intermediate_size: i32,     // 14336
    pub num_attention_heads: i32,   // 32
    pub num_key_value_heads: i32,   // 8 (GQA)
    pub vocab_size: i32,            // 100352
    pub rope_theta: f32,            // 250000.0
    pub rms_norm_eps: f32,          // 1e-5
    pub max_position_embeddings: i32, // 128000
}

#[derive(Debug, ModuleParameters, Quantizable)]
pub struct Phi4Attention {
    #[quantizable] #[param]
    pub q_proj: MaybeQuantized<Linear>,
    #[quantizable] #[param]
    pub k_proj: MaybeQuantized<Linear>,
    #[quantizable] #[param]
    pub v_proj: MaybeQuantized<Linear>,
    #[quantizable] #[param]
    pub o_proj: MaybeQuantized<Linear>,

    pub rope: Rope,
    pub n_heads: i32,
    pub n_kv_heads: i32,
    pub head_dim: i32,
    pub scale: f32,
}

#[derive(Debug, ModuleParameters, Quantizable)]
pub struct Phi4MLP {
    #[quantizable] #[param]
    pub gate_proj: MaybeQuantized<Linear>,
    #[quantizable] #[param]
    pub up_proj: MaybeQuantized<Linear>,
    #[quantizable] #[param]
    pub down_proj: MaybeQuantized<Linear>,
}

#[derive(Debug, ModuleParameters, Quantizable)]
pub struct Phi4Block {
    #[param]
    pub input_layernorm: RmsNorm,
    #[param]
    pub self_attn: Phi4Attention,
    #[param]
    pub post_attention_layernorm: RmsNorm,
    #[param]
    pub mlp: Phi4MLP,
}

#[derive(Debug, ModuleParameters, Quantizable)]
pub struct Phi4Model {
    #[quantizable] #[param]
    pub embed_tokens: MaybeQuantized<Embedding>,
    #[param]
    pub layers: Vec<Phi4Block>,
    #[param]
    pub norm: RmsNorm,
    #[quantizable] #[param]
    pub lm_head: MaybeQuantized<Linear>,
}
```

### 4.2 Enhanced Sampler

```rust
// NEW FILE: p04-inference-engine-core/src/sampler.rs

use mlx_rs::{Array, ops, random};

pub struct EnhancedSampler {
    pub temperature: f32,
    pub top_k: Option<usize>,
    pub top_p: Option<f32>,
    pub repeat_penalty: f32,
    pub repeat_window: usize,
}

impl EnhancedSampler {
    pub fn sample(&self, logits: &Array, recent_tokens: &[u32]) -> Result<u32, Exception> {
        let mut logits = logits.clone();

        // Apply repeat penalty
        if self.repeat_penalty != 1.0 && !recent_tokens.is_empty() {
            logits = self.apply_repeat_penalty(&logits, recent_tokens)?;
        }

        // Temperature scaling
        if self.temperature > 0.0 {
            logits = ops::divide(&logits, self.temperature)?;
        }

        // Top-K filtering
        if let Some(k) = self.top_k {
            logits = self.apply_top_k(&logits, k)?;
        }

        // Top-P (nucleus) filtering
        if let Some(p) = self.top_p {
            logits = self.apply_top_p(&logits, p)?;
        }

        // Sample from filtered distribution
        if self.temperature == 0.0 {
            logits.argmax(-1, None)?.item()
        } else {
            let probs = ops::softmax(&logits, -1)?;
            random::categorical(&probs, None, None, None)?.item()
        }
    }

    fn apply_top_k(&self, logits: &Array, k: usize) -> Result<Array, Exception> {
        let (values, indices) = ops::top_k(logits, k as i32, -1)?;
        let min_value = values.index((.., -1))?;
        ops::where_cond(
            &ops::lt(logits, &min_value)?,
            &Array::from(f32::NEG_INFINITY),
            logits,
        )
    }

    fn apply_top_p(&self, logits: &Array, p: f32) -> Result<Array, Exception> {
        let probs = ops::softmax(logits, -1)?;
        let sorted_indices = ops::argsort(&probs, -1)?;
        let sorted_probs = ops::take_along_axis(&probs, &sorted_indices, -1)?;
        let cumsum = ops::cumsum(&sorted_probs, -1, None, None)?;

        // Mask tokens beyond cumulative probability p
        let mask = ops::gt(&cumsum, p)?;
        ops::where_cond(&mask, &Array::from(f32::NEG_INFINITY), logits)
    }

    fn apply_repeat_penalty(&self, logits: &Array, recent: &[u32]) -> Result<Array, Exception> {
        let mut logits = logits.clone();
        for &token in recent.iter().rev().take(self.repeat_window) {
            let score = logits.index((.., token as i32))?.item::<f32>();
            let new_score = if score > 0.0 {
                score / self.repeat_penalty
            } else {
                score * self.repeat_penalty
            };
            // Update logits at token position
            logits = ops::scatter(&logits, &[token as i32], &Array::from(new_score), -1)?;
        }
        Ok(logits)
    }
}
```

### 4.3 Async HTTP Integration

```rust
// MODIFY: p02-http-server-core/src/lib.rs

use tokio::sync::mpsc;
use warp::sse::Event;

pub struct MlxInferenceHandler {
    model: Arc<tokio::sync::Mutex<Phi4Model>>,
    tokenizer: Arc<Tokenizer>,
    sampler: EnhancedSampler,
}

impl MlxInferenceHandler {
    pub async fn stream_generate(
        &self,
        prompt: String,
        max_tokens: usize,
    ) -> impl Stream<Item = Result<Event, warp::Error>> {
        let model = self.model.clone();
        let tokenizer = self.tokenizer.clone();
        let sampler = self.sampler.clone();

        let (tx, rx) = mpsc::unbounded_channel();

        // Spawn blocking MLX inference on dedicated thread
        tokio::task::spawn_blocking(move || {
            let rt = tokio::runtime::Handle::current();
            rt.block_on(async {
                let mut model = model.lock().await;

                // Tokenize
                let encoding = tokenizer.encode(&prompt, true).unwrap();
                let prompt_tokens = Array::from(encoding.get_ids());

                // Initialize cache
                let mut cache: Vec<Option<(Array, Array)>> = vec![None; 32];
                let mut recent_tokens = Vec::new();

                // Prefill
                let mut logits = model.forward(ModelInput {
                    inputs: &prompt_tokens,
                    cache: &mut cache,
                }).unwrap();

                // Generate tokens
                for i in 0..max_tokens {
                    // Sample next token
                    let next_token = sampler.sample(
                        &logits.index((.., -1, ..)).unwrap(),
                        &recent_tokens,
                    ).unwrap();

                    recent_tokens.push(next_token);

                    // Decode token
                    let text = tokenizer.decode(&[next_token], false).unwrap();

                    // Send SSE event
                    let event = Event::default()
                        .event("content_block_delta")
                        .json_data(serde_json::json!({
                            "type": "content_block_delta",
                            "delta": {"type": "text_delta", "text": text}
                        }))
                        .unwrap();

                    if tx.send(Ok(event)).is_err() {
                        break;
                    }

                    // Check for EOS
                    if next_token == tokenizer.token_to_id("<|endoftext|>").unwrap_or(0) {
                        break;
                    }

                    // Next forward pass (single token)
                    let input = Array::from(&[next_token][..]).reshape(&[1, 1]).unwrap();
                    logits = model.forward(ModelInput {
                        inputs: &input,
                        cache: &mut cache,
                    }).unwrap();

                    // Evaluate to materialize result
                    mlx_rs::transforms::eval(&[&logits]).unwrap();
                }

                // Send completion event
                let _ = tx.send(Ok(Event::default()
                    .event("message_stop")
                    .data("{\"type\":\"message_stop\"}")
                ));
            });
        });

        tokio_stream::wrappers::UnboundedReceiverStream::new(rx)
    }
}
```

---

## Part 5: Risk Assessment

### 5.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| mlx-rs API breaks | 40% | High | Pin version, abstract behind traits |
| Phi-4 port has bugs | 30% | High | Test against Python MLX output |
| Performance < 20 TPS | 20% | Medium | Profile, optimize hot paths |
| Memory > 5GB | 15% | Medium | Use quantization, limit context |
| GGUF conversion fails | 10% | Low | Use mlx-community safetensors |

### 5.2 Fallback Strategy

```rust
pub enum InferenceBackend {
    MlxRs(MlxEngine),           // Primary: Pure Rust + MLX
    MlxPython(PyO3Bridge),       // Fallback 1: Python MLX via FFI
    Candle(CandleEngine),        // Fallback 2: Pure Rust + Metal
}

impl InferenceBackend {
    pub fn best_available() -> Self {
        // Try mlx-rs first
        if let Ok(mlx) = MlxEngine::new() {
            return Self::MlxRs(mlx);
        }

        // Fallback to Python MLX
        if let Ok(bridge) = PyO3Bridge::new() {
            return Self::MlxPython(bridge);
        }

        // Last resort: Candle
        Self::Candle(CandleEngine::new().expect("Candle must work"))
    }
}
```

### 5.3 Production Readiness Checklist

Before v1.0.0:
- [ ] Phi-4 model passes correctness tests (compare to Python MLX)
- [ ] Token generation >= 20 TPS on M1 Pro
- [ ] Memory usage < 5GB under load
- [ ] All sampling modes work (temp, top-k, top-p)
- [ ] SSE streaming stable under concurrent requests
- [ ] No memory leaks after 1000 requests
- [ ] Graceful error handling (no panics)
- [ ] Warm start < 10 seconds

---

## Part 6: Comparison with Alternatives

### Why mlx-rs Over Alternatives?

| Factor | mlx-rs | Candle | mistral.rs | Python MLX |
|--------|--------|--------|------------|------------|
| **Apple Silicon Native** | âœ… Best | âš ï¸ Metal port | âš ï¸ Via Candle | âœ… Native |
| **Neural Engine Access** | âœ… Yes | âŒ No | âŒ No | âœ… Yes |
| **Pure Rust** | âœ… Yes | âœ… Yes | âœ… Yes | âŒ Python |
| **Unified Memory** | âœ… Native | âš ï¸ Emulated | âš ï¸ Emulated | âœ… Native |
| **M5+ Optimization** | âœ… Day 1 | âŒ Never | âŒ Never | âœ… Day 1 |
| **2027 Performance** | âœ… Best | âš ï¸ Plateau | âš ï¸ Plateau | âœ… Best |
| **Development Effort** | ðŸŸ¡ High | ðŸŸ¢ Low | ðŸŸ¢ Low | ðŸŸ¢ None |

### Decision Matrix

```
IF: Maximum future performance on Apple Silicon
    AND: Willing to invest 8-10 weeks development
    AND: Can accept v0.25.x API stability risks
THEN: Use mlx-rs

IF: Need production-ready today
    AND: Performance plateau acceptable
THEN: Use mistral.rs or Candle

IF: Fastest time to market
    AND: Python acceptable
THEN: Use Python MLX (current approach)
```

---

## Part 7: File Structure

### New/Modified Files

```
p04-inference-engine-core/
â”œâ”€â”€ Cargo.toml                        # Add mlx-rs, remove candle
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                        # Re-exports, feature flags
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ mod.rs                    # Backend trait
â”‚   â”‚   â”œâ”€â”€ mlx.rs                    # MLX-RS backend
â”‚   â”‚   â””â”€â”€ candle.rs                 # Candle fallback (optional)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ mod.rs                    # Model registry
â”‚   â”‚   â”œâ”€â”€ phi4.rs                   # Phi-4 implementation [NEW]
â”‚   â”‚   â””â”€â”€ config.rs                 # Model configs
â”‚   â”œâ”€â”€ sampler.rs                    # Enhanced sampling [NEW]
â”‚   â”œâ”€â”€ cache.rs                      # KV cache adapter
â”‚   â””â”€â”€ generator.rs                  # Token generation iterator
â””â”€â”€ tests/
    â”œâ”€â”€ phi4_tests.rs                 # Model unit tests
    â”œâ”€â”€ sampler_tests.rs              # Sampling tests
    â””â”€â”€ integration_tests.rs          # Full pipeline tests

p02-http-server-core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                        # Modified for MlxInferenceHandler
â”‚   â””â”€â”€ handlers/
â”‚       â””â”€â”€ inference.rs              # SSE streaming handler [NEW]
```

### Cargo.toml Changes

```toml
# p04-inference-engine-core/Cargo.toml

[dependencies]
# Core MLX-RS
mlx-rs = { path = "../refRepo/mlx-rs/mlx-rs", features = ["safetensors"] }
mlx-lm = { path = "../refRepo/mlx-rs/mlx-lm" }

# Tokenization
tokenizers = { version = "0.22", features = ["http"] }

# Async
tokio = { version = "1.40", features = ["full"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Remove these:
# candle-core = ...
# candle-nn = ...
# candle-transformers = ...
```

---

## Part 8: Timeline & Milestones

### 8-Week Implementation Schedule

| Week | Milestone | Deliverables |
|------|-----------|--------------|
| **1** | Foundation | Backend trait, Cargo.toml, mlx-rs integration |
| **2** | Phi-4 Core | Phi4Attention, Phi4MLP, Phi4Block structs |
| **3** | Phi-4 Complete | Phi4Model, forward pass, weight loading |
| **4** | Sampling | Top-K, Top-P, repeat penalty implementation |
| **5** | Generation | KV cache, Phi4Generator, token streaming |
| **6** | HTTP | Async wrapper, Warp SSE integration |
| **7** | Testing | Unit tests, integration tests, benchmarks |
| **8** | Optimization | Performance tuning, memory profiling |

### Success Criteria

```
v0.9.5 (Week 3): Phi-4 loads and generates (any quality)
v0.9.6 (Week 5): Streaming generation works
v0.9.7 (Week 6): HTTP SSE integration complete
v0.9.8 (Week 7): All tests passing
v0.9.9 (Week 8): Performance validated (>=20 TPS)
v1.0.0: Production release
```

---

## Conclusion

### Feasibility: YES

mlx-rs provides 90% of required primitives. The missing 10% (Phi-4 model, sampling, HTTP integration) can be implemented in 8-10 weeks by porting from Candle and adding async wrappers.

### Recommendation: PROCEED

The long-term benefits of mlx-rs (Neural Engine access, M5/M6/M7 optimization, Apple hardware co-evolution) outweigh the short-term development cost.

### Key Success Factors

1. **Port Phi-4 carefully** - Compare outputs to Python MLX for correctness
2. **Use safetensors** - Avoid GGUF complexity initially
3. **Abstract behind traits** - Enable backend switching if issues arise
4. **Test incrementally** - Validate each component before integration
5. **Monitor mlx-rs releases** - Track breaking changes

---

## Appendix: Reference Code Locations

### mlx-rs Source (to study)
```
refRepo/mlx-rs/mlx-rs/src/nn/mod.rs           # NN layers
refRepo/mlx-rs/mlx-rs/src/nn/quantized.rs     # Quantization
refRepo/mlx-rs/mlx-rs/src/fast.rs             # Optimized ops
refRepo/mlx-rs/mlx-lm/src/models/qwen3.rs     # Model reference
refRepo/mlx-rs/mlx-lm/src/cache.rs            # KV cache
refRepo/mlx-rs/examples/mistral/src/main.rs   # Inference example
```

### Candle Source (to port from)
```
refRepo/candle/candle-transformers/src/models/phi.rs           # Phi model
refRepo/candle/candle-transformers/src/models/quantized_phi.rs # Q-Phi
refRepo/candle/candle-transformers/src/generation/mod.rs       # Sampling
refRepo/candle/candle-examples/examples/quantized-phi/main.rs  # Example
```

---

*Thesis generated by Claude Code via 4-agent deep analysis*
*Target: Pure Rust + MLX for Apple Silicon LLM inference*
