#!/bin/bash
#
# Pensieve: Local LLM Launcher for Claude Code
#
# Two User Flows:
# 1. Happy Path: Everything exists -> Start Server -> Run Claude -> Cleanup
# 2. First Run: Missing components -> Download Binary/Model -> Start Server -> Run Claude
#

set -e

# --- Configuration ---
APP_NAME="pensieve"
DATA_DIR="${HOME}/.local/share/${APP_NAME}"
BIN_DIR="${HOME}/.local/bin"
MODEL_DIR="${DATA_DIR}/models/Phi-3-mini-128k-instruct-4bit"
MODEL_REPO="mlx-community/Phi-3-mini-128k-instruct-4bit"
SERVER_PORT=8765
SERVER_HOST="127.0.0.1"

# For development (since we don't have a real GH release yet), use local source
REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SRC_DIR="${REPO_ROOT}/src"

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() { echo -e "${BLUE}[Pensieve]${NC} $1"; }
success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1"; exit 1; }

# --- Cleanup Trap ---
cleanup() {
    echo ""
    log "Shutting down..."
    if [[ -n "$SERVER_PID" ]]; then
        kill "$SERVER_PID" 2>/dev/null || true
    fi
    # Also try to find any lingering server on the port
    lsof -ti:$SERVER_PORT | xargs kill -9 2>/dev/null || true
    exit 0
}
trap cleanup INT TERM EXIT

# --- Checks & Setup ---

# 1. Check Platform
if [[ "$(uname)" != "Darwin" ]] || [[ "$(uname -m)" != "arm64" ]]; then
    error "Pensieve requires macOS with Apple Silicon (ARM64)."
fi

# 2. Check/Install "Binary" (Python Environment)
# In a real release, this would download a compiled binary.
# Here, we ensure the Python environment is set up.
if ! command -v python3 &> /dev/null; then
    error "Python 3 is required."
fi

# Ensure we have the source files
if [[ ! -f "$SRC_DIR/server.py" ]]; then
    # This is where we would curl the binary if missing
    log "Downloading application..."
    # Simulation: We assume we are running from the repo. 
    # If not, we would clone/download here.
    error "Application source not found in $SRC_DIR"
fi

# 3. Check Dependencies (Lazy)
if ! python3 -c "import mlx_lm" &> /dev/null; then
    log "First run detected. Installing dependencies..."
    pip3 install -r "$SRC_DIR/requirements.txt" huggingface_hub > /dev/null
    success "Dependencies ready."
fi

# 4. Check/Download Model
if [[ ! -f "$MODEL_DIR/model.safetensors" ]]; then
    log "Model not found. Downloading Phi-3 (approx 2.5GB)..."
    mkdir -p "$MODEL_DIR"
    python3 -m huggingface_hub.cli download "$MODEL_REPO" --local-dir "$MODEL_DIR"
    success "Model downloaded."
else
    # Quick validity check
    if [[ $(find "$MODEL_DIR" -name "*.safetensors" -size +1G | wc -l) -eq 0 ]]; then
        warn "Model seems corrupt. Re-downloading..."
        python3 -m huggingface_hub.cli download "$MODEL_REPO" --local-dir "$MODEL_DIR"
    fi
fi

# --- Execution ---

# 5. Start Server
if curl -s -f "http://$SERVER_HOST:$SERVER_PORT/health" > /dev/null 2>&1; then
    log "Server is already running (attaching to existing instance)."
    # We don't set SERVER_PID so we don't kill it on exit
else
    log "Starting local inference server..."
    python3 "$SRC_DIR/server.py" --model-path "$MODEL_DIR" --port $SERVER_PORT > "$DATA_DIR/server.log" 2>&1 &
    SERVER_PID=$!
    
    # Wait for health check
    log "Waiting for model to load..."
    attempts=0
    while ! curl -s -f "http://$SERVER_HOST:$SERVER_PORT/health" > /dev/null 2>&1; do
        sleep 1
        attempts=$((attempts + 1))
        if [[ $attempts -gt 30 ]]; then
            cat "$DATA_DIR/server.log"
            error "Server failed to start."
        fi
    done
    success "Server ready."
fi

# 6. Run Claude
echo "----------------------------------------------------------------"
echo -e "${GREEN}Pensieve is active.${NC}"
echo "Running: claude"
echo "Session: Isolated (Local LLM)"
echo "----------------------------------------------------------------"

# Configure environment for this session only
export ANTHROPIC_BASE_URL="http://$SERVER_HOST:$SERVER_PORT"
export ANTHROPIC_API_KEY="pensieve-local-token"

# Check for claude
if ! command -v claude &> /dev/null; then
    error "'claude' command not found. Install with: npm install -g @anthropic-ai/claude-code"
fi

# Execute Claude
# When user Ctrl-C's Claude, the trap will catch it and kill the server
claude "$@"

# Trap will handle cleanup
